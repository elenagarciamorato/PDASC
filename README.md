# PDASC - Parametrizable Distributed Approximate Similarity search with Clustering
[![DOI](https://zenodo.org/badge/827397858.svg)](https://zenodo.org/doi/10.5281/zenodo.13220359)

<!-- * This project introduces PDASC (Parametrizable Distributed Approximate Similarity Search with Clustering), a parametrizable algorithm for ANN search that constructs a multilevel index using clustering techniques in an innovative way. By integrating clustering algorithms that inherently accommodate various distance functions, PDASC provides the adaptability needed to support arbitrary similarity measures.
-->
This project introduces PDASC (Parametrizable Distributed Approximate Similarity Search with Clustering), a parametrizable algorithm for approximate $k$-nearest neighbour search that combines an innovative multilevel index suitable for distributed environments with the flexibility to support arbitrary distance functions achieved through the integration of clustering algorithms that inherently accommodate them.

It constitutes a reliable ANN search method for large, high-dimensional datasets that cannot feasibly be processed on a single machine, as well as for those where selecting the most appropriate distance function is often non-trivial.

<!-- * For further detail, please refer to the associated publication. -->

![PDASC](benchmarks/figures/fig_multilayer_structure.png)


## Summary of features
__Scalable and Distributed__

PDASC implements an advanced multilevel indexing structure suitable for large, 
high-dimensional datasets, typically stored in distributed systems.

It operates through two stages
* The **Multilevel Structure Algorithm (MSA)** implements a bottom-up approach to construct a multilevel index structure suitable for distributed and scalable data indexing by employing clustering algorithms in an innovative way. 
* The **Neighbors Search Algorithm (NSA)** leverages the multilevel index structure generated by MSA to efficiently identify the $k$-nearest neighbours of a given query point $(q)$.


__Arbitrary Distances Support__

Although the only requirement for the clustering algorithm used to construct the index is that it partitions the data space into Voronoi regions and generates a representative prototype for each, PDASC's ability to support multiple distance functions is achieved by the integration of clustering algorithms in the index-building process which inherently accommodate them.

### Clustering Algorithms Supported:

| Algorithm     | Implementation                      | Supported Distances                                         |
|---------------|-------------------------------------|-------------------------------------------------------------|
| **k-means**   | `sklearn`: `sklearn.cluster.KMeans` | Euclidean                                                   |
| **k-medoids** | `kmedoids`: `kmedoids.fasterpam`    | cityblock, cosine, euclidean, l1, l2, manhattan, braycurtis, canberra, chebyshev, correlation, dice, hamming, jaccard, kulsinski, mahalanobis, minkowski, rogerstanimoto, russellrao, seuclidean, sokalmichener, sokalsneath, sqeuclidean, yule |

## Experimental Evaluation
A comprehensive evaluation of PDASC is presented through a series of experiments. In this study, $k$-medoids has been used due to its compatibility with a wide range of distances.

### Datasets:
| Dataset                                                       | Label       | N         | Dimensionality | High Sparsity | Data Type  | 
|---------------------------------------------------------------|-------------|-----------|----------------|---------------|------------|
| [**Municipalities**](https://doi.org/10.5281/zenodo.12759082) | X_muni      | 8,130     | 2              | No            | Geospatial |
| [**MNIST**](https://doi.org/10.5281/zenodo.12759284)          | X_MNIST     | 69,000    | 784            | Yes           | Image      |
| [**GLOVE**](https://doi.org/10.5281/zenodo.12759356)          | X_GLOVE     | 1,000,000 | 100            | No            | Text       |
| [**NYtimes**](https://doi.org/10.5281/zenodo.12760693)        | X_NYtimes   | 290,000   | 256            | No            | Text       |

### Distance Functions Employed:

| Distance      | API         | Equation                                                                                                                                                                                                                                                                                                                   |
|---------------|-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Euclidean** | `euclidean` | ![d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}](https://latex.codecogs.com/svg.latex?d%28x%2C%20y%29%20%3D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%20%28x_i%20-%20y_i%29%5E2%7D)                                                                                                                                                 |
| **Manhattan** | `manhattan` | ![d(x, y) = \sum_{i=1}^n \|x_i - y_i\|](https://latex.codecogs.com/svg.latex?d%28x%2C%20y%29%20%3D%20%5Csum_%7Bi%3D1%7D%5En%20%7Cx_i%20-%20y_i%7C)                                                                                                                                                                         |
| **Chebyshev** | `chebyshev` | ![d(x, y) = \max_i \|x_i - y_i\|](https://latex.codecogs.com/svg.latex?d%28x%2C%20y%29%20%3D%20%5Cmax_i%20%7Cx_i%20-%20y_i%7C)                                                                                                                                                                                             |
| **Minkowski** | `minkowski` | ![d(x, y) = \left( \sum_{i=1}^n \|x_i - y_i\|^p \right)^{\frac{1}{p}}](https://latex.codecogs.com/svg.latex?d%28x%2C%20y%29%20%3D%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5En%20%7Cx_i%20-%20y_i%7C%5Ep%20%5Cright%29%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D)                                                                             |
| **Cosine**    | `cosine`    | ![d(x, y) = 1 - \frac{\sum_{i=1}^n x_i y_i}{\sqrt{\sum_{i=1}^n x_i^2} \sqrt{\sum_{i=1}^n y_i^2}}](https://latex.codecogs.com/svg.latex?d%28x%2C%20y%29%20%3D%201%20-%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En%20x_i%20y_i%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%20x_i%5E2%7D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%20y_i%5E2%7D%7D) |
| **Haversine** | `haversine` | ![d(x, y) = 2r \arcsin\left(\sqrt{\sin^2\left(\frac{\phi_2 - \phi_1}{2}\right) + \cos(\phi_1) \cos(\phi_2) \sin^2\left(\frac{\lambda_2 - \lambda_1}{2}\right)}\right)](https://latex.codecogs.com/svg.latex?d%28x%2C%20y%29%20%3D%202r%20%5Carcsin%5Cleft%28%5Csqrt%7B%5Csin%5E2%5Cleft%28%5Cfrac%7B%5Cphi_2%20-%20%5Cphi_1%7D%7B2%7D%5Cright%29%20%2B%20%5Ccos%28%5Cphi_1%29%20%5Ccos%28%5Cphi_2%29%20%5Csin%5E2%5Cleft%28%5Cfrac%7B%5Clambda_2%20-%20%5Clambda_1%7D%7B2%7D%5Cright%29%7D%5Cright%29) |

### Approximate Algorithms Comparision:
PDASC performance is compared with the results obtained using two robust algorithms, PyNN and FLANN, which differ in their index construction strategies but are renowned for their scalability in approximate similarity searches on large datasets.
* __PyNNDescent__  a Python-based implementation of the NNDescent algorithm that efficiently performs nearest neighbour searches by building a graph-based index connecting each data point to its approximate nearest neighbours. 
* __FLANN__ an efficient, tree-based method that applies random projection and hierarchical partitioning to create index structures that accelerate search operations. 

### Experimental results
For each dataset, a pointplot illustrates the recall of three different algorithms across various distance metrics (Manhattan, Euclidean, Chebyshev, Cosine & Haversine) in approximate $10$ - nearest neighbor search.

![grafos4](benchmarks/figures/4grafos.png)

As shown by these pointplots, PDASC parametrised with the $k$-medoids clustering algorithm, consistently outperforms alternative indexing methods across all distance functions considered in our study.

These results not only validate PDASC as a reliable ANN search method for large, high-dimensional datasets that cannot feasibly be processed on a single machine and for those where selecting the optimal distance function is non-trivial, but also supports the core hypothesis of our research: employing distance functions beyond the Euclidean metric can yield superior results for approximate nearest-neighbour queries, depending on the dataset's specific characteristics.

## API description
This appendix is a guide to the usage of the PDASC Experiments Launcher and Benchmarking tool. Both are designed to facilitate the execution of approximate k-Nearest Neighbours searches by using the proposed method but also several SOTA related algorithms, then evaluating its performance on different contexts.

### Experiments Launcher
The `experiments_launcher.py` script facilitates the execution of the experiments whose parameters are described in configuration files (.ini). 

#### Usage:  
The script expects two arguments:
- Experiment: The name of the dataset whose experiments should be launched or a single .ini configuration file.
- Optional Filters: Optional filters to apply to the configuration files (if a directory is provided).<br />
`python3 -m benchmarks.experiments_launcher <dataset_name_or_ini_file> [optional_filters]`

#### Examples:  
- To run an experiment using a single .ini file:<br />
`python3 -m benchmarks.experiments_launcher test_knn_NYtimes_10_chebyshev_PDASC_tg1000_nc500_r30_kmedoids_fastkmedoids.ini`
- To run experiments for a dataset with optional filters:<br />
`python3 -m benchmarks.experiments_launcher NYtimes chebyshev PDASC`

### Performance Benchmarking
The `performance_benchmark.py` script facilitates the the performance evaluation of these experiments.

#### Usage:  
The script expects two arguments:
- Dataset: The name of the dataset whose experiments want to be benchmarked.
- Optional Filters: Optional filters to only show info about the desired experiments.
`python3 -m benchmarks.performance_benchmark <dataset_name> [optional_filters]`

#### Examples:  
- To benchmark experiments for a dataset:<br />
`python3 -m benchmarks.performance_benchmark NYtimes`

- To benchmark experiments for a dataset with optional filters:<br />
`python3 -m benchmarks.performance_benchmark NYtimes chebyshev PDASC`

## References

- **A parametrizable algorithm for distributed approximate similarity search with arbitrary distances**. Elena Garcia-Morato, Maria Jesus Algar, Cesar Alfaro, Felipe Ortega, Javier Gomez, Javier M. Moguerza. [arXiv:2405.13795](https://arxiv.org/abs/2405.13795)